{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00962289",
   "metadata": {},
   "source": [
    "# Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315a5eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pyspark.sql.functions as f\n",
    "from uuid import *\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from uuid import UUID\n",
    "import time_uuid \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848e998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6ee3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('spark.jars.packages','com.datastax.spark:spark-cassandra-connector_2.12:3.2.0')\\\n",
    "\t\t\t\t\t\t\t.config('spark.network.timeout','36000s')\\\n",
    "\t\t\t\t\t\t\t.config('spark.driver.memory','16g')\\\n",
    "\t\t\t\t\t\t\t.config('spark.executor.memory','1g')\\\n",
    "\t\t\t\t\t\t\t.config('spark.driver.maxResultSize','16g')\\\n",
    "\t\t\t\t\t\t\t.config('spark.executor.heartbeatInterval','18000s')\\\n",
    "\t\t\t\t\t\t\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd324a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb14c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    spark_time = df.select('create_time').collect()\n",
    "    normal_time = []\n",
    "    for i in range(len(spark_time)):\n",
    "        a = time_uuid.TimeUUID(bytes=UUID(df.select('create_time').collect()[i][0]).bytes).get_datetime().strftime(('%Y-%m-%d %H:%M:%S'))\n",
    "        normal_time.append(a)\n",
    "    spark_timeuuid = []\n",
    "    for i in range(len(spark_time)):\n",
    "        spark_timeuuid.append(spark_time[i][0])\n",
    "    time_data = spark.createDataFrame(zip(spark_timeuuid,normal_time),['create_time','ts'])\n",
    "    result = df.join(time_data,['create_time'],'inner').drop(df.ts)\n",
    "    result = result.select('create_time','ts','job_id','custom_track','bid','campaign_id','group_id','publisher_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1b5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_clicks(df):\n",
    "    click = df.filter(df.custom_track == 'click')\n",
    "    click = click.na.fill({'bid':0})\n",
    "    click = click.na.fill({'job_id':0})\n",
    "    click = click.na.fill({'campaign_id':0})\n",
    "    click = click.na.fill({'group_id':0})\n",
    "    click = click.na.fill({'publisher_id':0})\n",
    "    click.createOrReplaceTempView('clicks')\n",
    "    click_output = spark.sql(\"\"\"select job_id, ts as date, hour(ts) as hour, publisher_id, campaign_id, group_id, avg(bid) as bid_set,count(*) as click, sum(bid) as spend_hour \n",
    "                                from clicks\n",
    "                                group by job_id, ts, hour(ts), publisher_id, campaign_id, group_id\"\"\")\n",
    "    return click_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ee6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_conversion(df):\n",
    "    conversion = df.filter(df.custom_track == 'conversion')\n",
    "    conversion = conversion.na.fill({'job_id':0})\n",
    "    conversion = conversion.na.fill({'campaign_id':0})\n",
    "    conversion = conversion.na.fill({'group_id':0})\n",
    "    conversion = conversion.na.fill({'publisher_id':0})\n",
    "    conversion.createOrReplaceTempView('conversion')\n",
    "    conversion_output = spark.sql(\"\"\"select job_id, ts as date, hour(ts) as hour, publisher_id, campaign_id, group_id, count(*) as conversion\n",
    "                                    from conversion\n",
    "                                    group by job_id, ts, hour(ts), publisher_id, campaign_id, group_id\"\"\")\n",
    "    return conversion_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bda8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_qualified(df):    \n",
    "    qualified_data = df.filter(df.custom_track == 'qualified')\n",
    "    qualified_data = qualified_data.na.fill({'job_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'publisher_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'group_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'campaign_id':0})\n",
    "    qualified_data.createOrReplaceTempView('qualified')\n",
    "    qualified_output = spark.sql(\"\"\"select job_id , ts as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as qualified  \n",
    "                                    from qualified\n",
    "                                    group by job_id , ts , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return qualified_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16eecd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_unqualified(df):\n",
    "    unqualified_data = df.filter(df.custom_track == 'unqualified')\n",
    "    unqualified_data = unqualified_data.na.fill({'job_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'publisher_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'group_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'campaign_id':0})\n",
    "    unqualified_data.createOrReplaceTempView('unqualified')\n",
    "    unqualified_output = spark.sql(\"\"\"select job_id , ts as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as unqualified  \n",
    "                                    from unqualified\n",
    "                                    group by job_id , ts , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return unqualified_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f114c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_data(click_output,conversion_output,qualified_output,unqualified_output):\n",
    "    final_data = click_output.join(conversion_output,['job_id', 'date', 'hour',  'publisher_id', 'campaign_id', 'group_id'],'full')\\\n",
    "                .join(qualified_output,['job_id', 'date', 'hour',  'publisher_id', 'campaign_id', 'group_id'],'full')\\\n",
    "                .join(unqualified_output,['job_id', 'date', 'hour',  'publisher_id', 'campaign_id', 'group_id'],'full')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083c55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cassandra_data(df):\n",
    "    clicks_output = calculating_clicks(df)\n",
    "    conversion_output = calculating_conversion(df)\n",
    "    qualified_output = calculating_qualified(df)\n",
    "    unqualified_output = calculating_unqualified(df)\n",
    "    final_data = process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd15c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_data(url,driver,user,password):\n",
    "    sql = \"\"\"(select id as job_id, company_id FROM job) as company\"\"\"\n",
    "    company = spark.read.format('jdbc').options(url=url, driver=driver, dbtable=sql, user=user, password=password).load()\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b395fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_to_mysql(output):\n",
    "    output = output.withColumn('last_update_times',lit(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))).withColumn('sources',lit('Cassandra'))\n",
    "    final_output = output.select('job_id','date','hour','publisher_id','company_id','campaign_id','group_id','unqualified','qualified','conversion','click','bid_set','spend_hour','last_update_times','sources')\n",
    "    final_output.write.format('jdbc')\\\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3310/de_learning\") \\\n",
    "    .option(\"dbtable\", \"events_etl\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"sa\") \\\n",
    "    .save()\n",
    "    print('+--------------------------------------------+')\n",
    "    return print('| Data imported successfully                 |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2e4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_time_cassandra():\n",
    "    cassandra_time = spark.read.format('org.apache.spark.sql.cassandra').options(table = 'tracking', keyspace = 'de_learning').load().agg({'ts':'max'}).take(1)[0][0]\n",
    "    return cassandra_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd92305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_time_mysql(url, driver, user, password):\n",
    "    sql = \"\"\"(select max(last_update_times) from events_etl) as mysqltime\"\"\"\n",
    "    mysql_time = spark.read.format('jdbc').options(url=url, driver=driver, dbtable=sql, user=user, password=password).load().take(1)[0][0]\n",
    "    if mysql_time is None:\n",
    "        lated_time = '1999-12-31 23:59:59'\n",
    "    else :\n",
    "        lated_time = mysql_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return lated_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d990beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_task(mysql_time):\n",
    "    driver = 'com.mysql.cj.jdbc.Driver'\n",
    "    url = 'jdbc:mysql://localhost:3310/de_learning'\n",
    "    user = 'root'\n",
    "    password = 'sa'\n",
    "    print('+--------------------------------------------+')\n",
    "    print(\"| Retrieve data from Cassandra               |\")\n",
    "    df = spark.read.format('org.apache.spark.sql.cassandra').options(table = 'tracking', keyspace = 'de_learning').load().where(col('ts') >= mysql_time)\n",
    "    print('+--------------------------------------------+')\n",
    "    print(\"| Process data...                            |\")\n",
    "    data = process_data(df)\n",
    "    cassandra_output = process_cassandra_data(data)\n",
    "    print('+--------------------------------------------+')\n",
    "    print(\"| Retrieve Company_ID                        |\")\n",
    "    company_data = get_company_data(url,driver,user,password)\n",
    "    print('+--------------------------------------------+')\n",
    "    print(\"| Finalizing Output                          |\")\n",
    "    final_output = cassandra_output.join(company_data,'job_id','left')\n",
    "    import_to_mysql(final_output)\n",
    "    print('+--------------------------------------------+')\n",
    "    return print('| Task Completed!                            |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2099a41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:22:35.424534   |\n",
      "+--------------------------------------------+\n",
      "| Retrieve data from Cassandra               |\n",
      "+--------------------------------------------+\n",
      "| Process data...                            |\n",
      "+--------------------------------------------+\n",
      "| Retrieve Company_ID                        |\n",
      "+--------------------------------------------+\n",
      "| Finalizing Output                          |\n",
      "+--------------------------------------------+\n",
      "| Data imported successfully                 |\n",
      "+--------------------------------------------+\n",
      "| Task Completed!                            |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:32:58.101410     |\n",
      "+--------------------------------------------+\n",
      "| Job take 622.676876 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:33:28.101679   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:33:28.332462     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.230783 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:33:58.345426   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:33:58.609405     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.263979 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:34:28.616989   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:34:28.834110     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.217121 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:34:58.838664   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:34:59.055793     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.217129 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:35:29.070929   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:35:29.265408     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.194479 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:35:59.277336   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:35:59.478588     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.201252 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------------------------------------------+\n",
      "| Building Data Pipeline Task                |\n",
      "+--------------------------------------------+\n",
      "+--------------------------------------------+\n",
      "| Time start at 2023-05-07 00:36:29.479285   |\n",
      "+--------------------------------------------+\n",
      "| New Data not found!                        |\n",
      "+--------------------------------------------+\n",
      "| Time end at 2023-05-07 00:36:29.710843     |\n",
      "+--------------------------------------------+\n",
      "| Job take 0.231558 seconds to execute! \n",
      "+--------------------------------------------+\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+--------------------------------------------+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('+--------------------------------------------+')\n",
    "    print('| Building Data Pipeline Task                |')\n",
    "    print('+--------------------------------------------+')\n",
    "    driver = 'com.mysql.cj.jdbc.Driver'\n",
    "    url = 'jdbc:mysql://localhost:3310/de_learning'\n",
    "    user = 'root'\n",
    "    password = 'sa'\n",
    "    timestart = datetime.datetime.now()\n",
    "    print('+--------------------------------------------+')\n",
    "    print('| Time start at {}   |'.format(timestart))\n",
    "    last_cassandra_time = get_latest_time_cassandra()\n",
    "    last_mysql_time = get_latest_time_mysql(url, driver, user, password)\n",
    "    if last_cassandra_time > last_mysql_time:\n",
    "        main_task(last_mysql_time)\n",
    "    else:\n",
    "        print('+--------------------------------------------+')\n",
    "        print('| New Data not found!                        |')\n",
    "    timeend = datetime.datetime.now()\n",
    "    print('+--------------------------------------------+')\n",
    "    print('| Time end at {}     |'.format(timeend))\n",
    "    print('+--------------------------------------------+')\n",
    "    print('| Job take {} seconds to execute! '.format((timeend - timestart).total_seconds()))\n",
    "    print('+--------------------------------------------+')\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0560ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-06 23:55:17'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_cassandra_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc59dea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-07 00:14:44'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_mysql_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
